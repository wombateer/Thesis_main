{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bir3XIgPD2J"
      },
      "source": [
        "# Feature Evaluation and Selection for Random Forest Classifier of Symptomatic and Non-Symptomatic Sweet Chestnut"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0swJenU6PD2K"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OOSXsYsBPD2L"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_selection import RFE\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIBYXuekdKes"
      },
      "source": [
        "### Feature Selection with Leave-Location-Out Cross-Validation (LLO CV) and Recursive Feature Elimination (RFE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7FxEugzPD2N"
      },
      "source": [
        "#### Leave-Location-Out Cross-Validation (LLO CV) for Model Testing\n",
        "\n",
        "To test and validate the performance of the Random Forest (RF) classifier, Leave-Location-Out Cross-Validation (LLO CV) was used. For this, I defined 9 folds based on a visual assesment of the spatial distribution of the given tree samples. From each fold, 10 samples are held out for the testset. The remaining samples are used for the trainset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E90ES_CPD2N"
      },
      "outputs": [],
      "source": [
        "# Loading input file with sampled tree data and conversion into dataframe\n",
        "# The samples are labelled with its corresponding spatial fold (1 to 9)\n",
        "df = pd.read_csv(\"input/input_data.csv\", sep=\";\")\n",
        "\n",
        "trainSets_List = []\n",
        "testSets_List = []\n",
        "\n",
        "# Iteratively assign 10 samples from each spatial fold for the testset and the remaining samples to the trainset\n",
        "for i in range(1,10):\n",
        "    temp_df = pd.DataFrame(df[(df['Spat_fold']==i)]).drop(columns=['Spat_fold']).sample(frac=1).reset_index(drop=True)\n",
        "    testSets_List.append(temp_df.tail(10))\n",
        "    trainSets_List.append(temp_df.iloc[:-10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BRGk-F-gDnY"
      },
      "source": [
        "#### Pre-Selection of Features\n",
        "\n",
        "For performance efficiency, I preselect 32 features from the total of 64 available features with one single Recursive Feature Elimination (RFE) run. For performing RFE, the use of an estimator, in this case the Random Forest (RF) model, is required. By reducing the feature set to 32 input features, it became computationally feasible to conduct 180 different feature combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_KSOsAqqNAP"
      },
      "outputs": [],
      "source": [
        "# Initialise the RF classifier \n",
        "selection_model = RandomForestClassifier()\n",
        "\n",
        "# Apply RFE with the RF classifier as an estimator\n",
        "# Select the 32 most important features by removing 1 feature per iteration\n",
        "rfe = RFE(estimator=selection_model, n_features_to_select=32, step=1)\n",
        "\n",
        "# Fit the RF model with the trainset, while removing the labelled classes \"symptomatic\" and \"non-symptomatic\"\n",
        "rfe.fit(pd.concat(trainSets_List).drop('Class', axis=1), pd.concat(trainSets_List)['Class'])\n",
        "\n",
        "# Save preselected input features in a dataframe\n",
        "preselected_features = pd.concat(trainSets_List).drop('Class', axis=1).columns[rfe.support_]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Find Optimal Number and Combination of Features with Recursive Feature Elimination (RFE) and Leave-Location-Out Cross-Validation (LLO CV)\n",
        "\n",
        "Using a large number of features as input to the RF model can lead to high correlations between features, which may bias the evaluation of feature importances (Dobrinić et al., 2022). By reducing the feature set to 32, I aim to minimise such correlations and ensure a more accurate assessment of feature relevance. Thus, I aim to find the most relevant feature combination to optimise the performance of the Random Forest (RF) model. For this, I need to evaluate which and how many features should be used. I focus on evaluating subsets of 6 to 23 features (n = 6, 7, ..., 23). For each number of features n, 10 different feature combinations are being built out from the pre-selection of the 32 most relevant features. This processing allows for an evaluation of the model’s performance across different numbers of features with computational efficiency and the potential for improved classification accuracy.\n",
        "\n",
        "Source:  Dobrinić, D., Gašparović, M., & Medak, D. (2022). Evaluation of Feature Selection Methods for Vegetation Mapping Using Multitemporal Sentinel Imagery. The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLIII-B3-2022, 485–491. https://doi.org/10.5194/isprs-archives-XLIII-B3-2022-485-2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bQqmWP0iPD2O",
        "outputId": "59d7955b-35b7-4c91-e7be-df6124790393"
      },
      "outputs": [],
      "source": [
        "best_accuracy = 0\n",
        "best_num_features = 0\n",
        "results = []\n",
        "\n",
        "# Set range for number of features [6, ..., 23]\n",
        "range_features_rfe = range(6, 24)\n",
        "\n",
        "# Perform RFE before LLO CV to select input features\n",
        "for num_features in range_features_rfe:\n",
        "\n",
        "    # Initialise list to store all 180 (10 combinations * 18 number of features) generated feature c0mbinations\n",
        "    all_selected_features = []\n",
        "\n",
        "    # Perform RFE 10 times to generate different feature combinations for n input features [6, ..., 23] and store selected features\n",
        "    # A random seed is used to get variability in testset \n",
        "    for _ in range(10):\n",
        "        random_seed = random.randint(0, 10000)\n",
        "        selection_model = RandomForestClassifier(random_state=random_seed)\n",
        "        rfe = RFE(estimator=selection_model, n_features_to_select=num_features, step=1)\n",
        "        rfe.fit(pd.concat(trainSets_List)[preselected_features], pd.concat(trainSets_List)['Class'])\n",
        "        selected_features = pd.concat(trainSets_List)[preselected_features].columns[rfe.support_]\n",
        "        all_selected_features.append(selected_features)\n",
        "\n",
        "    # Initialise list to store mean accuracy for each combination of features\n",
        "    mean_accuracies = []\n",
        "\n",
        "    # Loop through each RFE-selected feature combination, train a RF model and validate its performance with LLO CV\n",
        "    for selected_features in all_selected_features:\n",
        "        fold_accuracies = []\n",
        "        # The defaultdict-function simplifies the code as checking of existing keys and initialisation of new lists is not necessary anymore\n",
        "        feature_importance_tracker = defaultdict(list) \n",
        "\n",
        "        # Perform LLO CV\n",
        "        for i in range(9):\n",
        "            # Define the validation and training sets\n",
        "            df_val = trainSets_List[i]\n",
        "            X_val = df_val.loc[:, selected_features]\n",
        "            y_val = df_val['Class']\n",
        "\n",
        "            df_train = pd.concat([trainSets_List[j] for j in range(9) if j != i])\n",
        "            X_train = df_train.loc[:, selected_features]\n",
        "            y_train = df_train['Class']\n",
        "\n",
        "            # Train the model with selected features\n",
        "            classifier_model = RandomForestClassifier()\n",
        "            classifier_model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = classifier_model.predict(X_val)\n",
        "\n",
        "            # Calculate accuracy for each fold and store it\n",
        "            accuracy = accuracy_score(y_val, y_pred)\n",
        "            fold_accuracies.append(accuracy)\n",
        "\n",
        "            # Track feature importances for selected features\n",
        "            for feature, importance in zip(selected_features, classifier_model.feature_importances_):\n",
        "                feature_importance_tracker[feature].append(importance)\n",
        "\n",
        "        # Calculate the mean accuracy across all folds for this feature combination \n",
        "        mean_accuracy = np.mean(fold_accuracies)\n",
        "        mean_accuracies.append(mean_accuracy)\n",
        "\n",
        "        # If the current model has a better mean accuracy, update the best performing model and accuracy parameter\n",
        "        if mean_accuracy > best_accuracy:\n",
        "            best_accuracy = mean_accuracy\n",
        "            best_num_features = num_features\n",
        "\n",
        "        # Store the collected metrics (different accuracies) for each feature combination\n",
        "        averaged_importances = {feature: np.mean(importances) for feature, importances in feature_importance_tracker.items()}\n",
        "        results.append({\n",
        "            'num_features': num_features,\n",
        "            'mean_accuracy': mean_accuracy,\n",
        "            'feature_importances': averaged_importances,\n",
        "            'CV_accuracies': fold_accuracies,\n",
        "            'selected_features': selected_features\n",
        "        })\n",
        "\n",
        "        print(f\"RFE with {num_features} features - Mean CV Accuracy: {mean_accuracy:.4f}\")\n",
        "\n",
        "    print(f\"Best accuracy for {num_features} features across 10 RFE runs: {max(mean_accuracies):.4f}\")\n",
        "\n",
        "print(f\"Best CV Accuracy: {best_accuracy:.4f} with {best_num_features} features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMEFXrpVPD2P"
      },
      "source": [
        "#### Results of Feature Evaluation\n",
        "\n",
        "This section provides an overview of the feature evaluation process by displaying a dataframe that contains all the collected results. The dataframe includes the performances of a RF model with 10 different feature combinations as input data for each selected number of features [6, ..., 23]. For each feature combination, the following information is presented:\n",
        "\n",
        "- The **numer of features** selected for training the model\n",
        "- The **mean accuracy** of the LLO CV\n",
        "- The **individual LLO CV accuracies** for each fold\n",
        "- The **selected features** for that particular combination\n",
        "- The **sorted feature importances** for that particular combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "CYo6kVsHPD2P",
        "outputId": "c5c17d77-ce78-4e5f-80e4-5bb30cf5fcb3"
      },
      "outputs": [],
      "source": [
        "# Convert results list into a dataframe for better displaying options\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Rank feature importances for better overview\n",
        "results_df['sorted_feature_importances'] = results_df['feature_importances'].apply(\n",
        "    lambda x: {k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)})\n",
        "\n",
        "results_df.drop('feature_importances', inplace=True)\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "kl5-Zg6t90_l",
        "outputId": "7b88cdc7-bce0-496c-f3c3-4ff268e4132e"
      },
      "outputs": [],
      "source": [
        "# Extract and display details of best performing model\n",
        "best_row = results_df[results_df['num_features'] == best_num_features]\n",
        "importances = best_row['sorted_feature_importances'].values[0]\n",
        "CV_accuracies = best_row['CV_accuracies'].values[0]\n",
        "\n",
        "importances_df = pd.DataFrame(importances.items(), columns=['Feature', 'Importance']).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "CV_accuracies_df = pd.DataFrame(CV_accuracies, columns=['CV_Accuracy'])\n",
        "\n",
        "CV_acc_num_features_df = results_df[['num_features', 'mean_accuracy']]\n",
        "\n",
        "display(importances_df)\n",
        "display(CV_accuracies_df)\n",
        "display(CV_acc_num_features_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhwIuYBBPD2P"
      },
      "source": [
        "### Final Random Forest (RF) Model Training\n",
        "With the best performing feature combination evaluated in the combined approach of RFE and LLO CV, I train the final RF model with the given trainset for the classification task of my thesis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpRaicF1rycS",
        "outputId": "3e988b03-1d0d-43ea-b501-fb5c4611105e"
      },
      "outputs": [],
      "source": [
        "# Putting tThe best performing feature combination from LLO CV into a list\n",
        "top_importances = importances_df['Feature'].head(best_num_features).tolist()\n",
        "top_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgm1WRuLPD2Q",
        "outputId": "9594f472-d34f-4770-f7ac-d5b4a776bfc4"
      },
      "outputs": [],
      "source": [
        "# Define class names to corresponding integers \n",
        "class_mapping = {'non-symptomatic': 0, 'symptomatic': 1}\n",
        "\n",
        "# Define the testing and training sets and map class names to integers\n",
        "trainSet = pd.concat(trainSets_List)\n",
        "y_train = trainSet['Class'].map(class_mapping)\n",
        "X_train = trainSet.drop('Class', axis=1)\n",
        "\n",
        "testSet = pd.concat(testSets_List, ignore_index=True)\n",
        "y_Test = testSet['Class'].map(class_mapping) \n",
        "X_Test = testSet.drop('Class', axis=1)\n",
        "\n",
        "# Reduce testing and training sets to only the selected input features\n",
        "selected_features = top_importances\n",
        "X_train_rfe = X_train.loc[:, selected_features]\n",
        "X_test_rfe = X_Test.loc[:, selected_features]\n",
        "\n",
        "# Train the model with the final selected input features\n",
        "final_classifier = RandomForestClassifier()\n",
        "final_classifier.fit(X_train_rfe, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = final_classifier.predict(X_test_rfe)\n",
        "\n",
        "# Check class distributions (Braucht es dies?)\n",
        "print(\"True Class Distribution:\")\n",
        "print(y_Test.value_counts())\n",
        "print(\"Predicted Class Distribution:\")\n",
        "print(pd.Series(y_pred).value_counts())\n",
        "\n",
        "# Calculate the accuracy on the testing set for getting the final accuracy score\n",
        "accuracy_TestSet = accuracy_score(y_Test, y_pred)\n",
        "\n",
        "# Calculate the class-specific accuracy metrics on the testing set for getting the final class-wise accuracy scores\n",
        "precision_TestSet_class = precision_score(y_Test, y_pred, average=None, labels=[0, 1], zero_division=0)\n",
        "recall_TestSet_class = recall_score(y_Test, y_pred, average=None, labels=[0, 1], zero_division=0)\n",
        "f1_TestSet_class = f1_score(y_Test, y_pred, average=None, labels=[0, 1], zero_division=0)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test Set Accuracy: {accuracy_TestSet:.4f}\")\n",
        "print(\"\")\n",
        "print(f\"Symptomatic (Class 1) Precision: {precision_TestSet_class[1]:.4f}\")\n",
        "print(f\"Symptomatic (Class 1) Recall: {recall_TestSet_class[1]:.4f}\")\n",
        "print(f\"Symptomatic (Class 1) F1 Score: {f1_TestSet_class[1]:.4f}\")\n",
        "print(\"\")\n",
        "print(f\"Non-Symptomatic (Class 0) Precision: {precision_TestSet_class[0]:.4f}\")\n",
        "print(f\"Non-Symptomatic (Class 0) Recall: {recall_TestSet_class[0]:.4f}\")\n",
        "print(f\"Non-Symptomatic (Class 0) F1 Score: {f1_TestSet_class[0]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBPUHW5sPD2Q"
      },
      "source": [
        "### Feature Evaluation of Final Random Forest (RF) Model\n",
        "\n",
        "Again, I export the feature importances of the final RF model for later analysis in my thesis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "nToCDAbPPD2R",
        "outputId": "493b4f39-a69f-41fe-ff8f-afc30733baa4"
      },
      "outputs": [],
      "source": [
        "# Creating sorted dataframe of feature importances of the final model and exporting it to a csv-file\n",
        "final_features = final_classifier.feature_importances_\n",
        "final_features_df = pd.DataFrame({\n",
        "    'Feature': X_train_rfe.columns,\n",
        "    'Importance (Best Model)': final_features})\n",
        "final_features_df.sort_values(by='Importance (Best Model)', ascending=False, inplace=True)\n",
        "display(final_features_df)\n",
        "final_features_df.to_csv('output/final_feature_importances.csv')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
